# **Sound Bridgeï¼šassociation Egocentric and Exocentric videos via Audio Cues**<br>
Almost thereğŸ¤­



## ğŸ“¢ News
**[2025.3.10]** The code and dataset of related tasks has been released.

**[2025.3.5]** The repository is public.

**[2025.2.28]** The repository is created.

<a name="installation"></a>
## âš™ï¸ Installation
1. Clone the repository from GitHub.

```shell
git clone https://github.com/shhuangcoder/SoundBridge.git
cd SoundBridge
```

2. Create conda environment.

```shell
conda create -n SoundBridge python=3.8
conda activate SoundBridge
```


3. Download the packages
```shell
pip install -r requirements.txt
```

## ğŸ—‚ï¸ Dataset
For the dataset, we provide the corresponding download link. Please follow the download instructions provided in the link to download the dataset.<br>
EgoExoLearn: https://github.com/OpenGVLab/EgoExoLearn<br>
CharadesEgo: https://prior.allenai.org/projects/charades-ego<br>
You need to first extract the audio file (.wav) from the video and then use the BEATs model for audio feature extraction. Please call the BEATs model according to its requirements. [BEATS](https://github.com/microsoft/unilm.git)

### The prepared dataset should be in the following structure.
```
.
â”œâ”€â”€ SoundBridge
â”‚Â Â  â”œâ”€â”€ model
â”‚Â Â  â””â”€â”€ data
â”‚Â Â  â””â”€â”€ results
â”‚Â Â  â””â”€â”€ config
â”‚Â Â  â””â”€â”€ annotations
â”‚Â Â  â””â”€â”€ utils
â”‚Â Â  â””â”€â”€ function
â”œâ”€â”€ datasets
â”‚Â Â  â””â”€â”€ Charades_Ego
â”‚   â”‚   â””â”€â”€ videos
â”‚   â”‚  â””â”€â”€ audios
â”‚ Â Â â””â”€â”€ EgoExoLearn
â”‚   â”‚   â””â”€â”€ videos
â”‚   â”‚   â””â”€â”€ audios
â”œâ”€â”€ ckpt
â”‚ Â Â â””â”€â”€ Cha.pth
â”‚ Â Â â””â”€â”€ EgoExo.pth
â”œâ”€â”€README.md
â””â”€â”€ Â·Â·Â·
```


## ğŸª Checkpoint
* You can download from [huggingface](https://huggingface.co/Sihong/SoundBridge)


